{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' ', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "\n",
      "\n",
      "(' ', ' ', ' ')\n",
      "('X', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "\n",
      "\n",
      "(' ', ' ', ' ')\n",
      "('X', 'O', ' ')\n",
      "(' ', ' ', ' ')\n",
      "\n",
      "\n",
      "(' ', ' ', ' ')\n",
      "('X', 'O', ' ')\n",
      "(' ', 'X', ' ')\n",
      "\n",
      "\n",
      "('O', ' ', ' ')\n",
      "('X', 'O', ' ')\n",
      "(' ', 'X', ' ')\n",
      "\n",
      "\n",
      "('O', ' ', ' ')\n",
      "('X', 'O', ' ')\n",
      "(' ', 'X', 'X')\n",
      "\n",
      "\n",
      "('O', ' ', ' ')\n",
      "('X', 'O', ' ')\n",
      "('O', 'X', 'X')\n",
      "\n",
      "\n",
      "('O', ' ', 'X')\n",
      "('X', 'O', ' ')\n",
      "('O', 'X', 'X')\n",
      "\n",
      "\n",
      "('O', ' ', 'X')\n",
      "('X', 'O', 'O')\n",
      "('O', 'X', 'X')\n",
      "\n",
      "\n",
      "('O', 'X', 'X')\n",
      "('X', 'O', 'O')\n",
      "('O', 'X', 'X')\n",
      "\n",
      "\n",
      "It's a draw.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "class TicTacToeEnv:\n",
    "    def __init__(self):\n",
    "        self.states = list(itertools.product([\"X\", \"O\", \" \"], repeat=9))\n",
    "\n",
    "    @staticmethod\n",
    "    def actions(state):\n",
    "        return [i for i, s in enumerate(state) if s == \" \"]\n",
    "\n",
    "    @staticmethod\n",
    "    def transition_model(state, action, player):\n",
    "        state_list = list(state)\n",
    "        state_list[action] = player\n",
    "        return tuple(state_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def reward(state, player):\n",
    "        win_positions = [(0, 1, 2), (3, 4, 5), (6, 7, 8), (0, 3, 6), (1, 4, 7), (2, 5, 8), (0, 4, 8), (2, 4, 6)]\n",
    "        if player == \"X\":\n",
    "            opponent = \"O\"\n",
    "        else:\n",
    "            opponent = \"X\"\n",
    "\n",
    "        for pos in win_positions:\n",
    "            if state[pos[0]] == state[pos[1]] == state[pos[2]] == player:\n",
    "                return 1  # Reward for winning\n",
    "\n",
    "        for pos in win_positions:\n",
    "            if state[pos[0]] == state[pos[1]] == state[pos[2]] == opponent:\n",
    "                return -1  # Negative reward for losing\n",
    "\n",
    "        if \" \" not in state:\n",
    "            return 0  # Reward for a draw\n",
    "\n",
    "        # Modify the reward for other scenarios\n",
    "        if state[4] == player:\n",
    "            return 0.5  # Encourage taking the center position\n",
    "\n",
    "        corner_positions = [0, 2, 6, 8]\n",
    "        if any(state[i] == player for i in corner_positions):\n",
    "            return -0.1  # Discourage taking corners\n",
    "\n",
    "        edge_positions = [1, 3, 5, 7]\n",
    "        if any(state[i] == player for i in edge_positions):\n",
    "            return -0.1  # Slightly discourage taking edges\n",
    "\n",
    "        return 0  # Default reward for other situations\n",
    "\n",
    "    @staticmethod\n",
    "    def is_terminal(state):\n",
    "        return TicTacToeEnv.reward(state, \"X\") == 1 or TicTacToeEnv.reward(state, \"O\") == 1 or \" \" not in state\n",
    "\n",
    "    @staticmethod\n",
    "    def get_available_actions(state):\n",
    "        return [i for i, s in enumerate(state) if s == \" \"]\n",
    "\n",
    "class TicTacToeTD:\n",
    "    def __init__(self, game, alpha=0.01, gamma=0.99, epsilon=0.1):\n",
    "        self.q_values = {}  # Dictionary to store state-action values\n",
    "        self.game = game\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.replay_buffer = []  # List to store states where the AI lost\n",
    "\n",
    "    @staticmethod\n",
    "    def actions(state):\n",
    "        return [i for i, s in enumerate(state) if s == \" \"]\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Check if this state is in the replay buffer\n",
    "        if state in self.replay_buffer:\n",
    "            # Avoid any action that leads to the state\n",
    "            valid_actions = [a for a in self.actions(state) if self.game.transition_model(state, a, \"X\") not in self.replay_buffer]\n",
    "            if valid_actions:\n",
    "                return random.choice(valid_actions)\n",
    "\n",
    "        # First, check if a winning move is possible\n",
    "        for a in self.actions(state):\n",
    "            next_state = self.game.transition_model(state, a, \"X\")\n",
    "            if self.game.reward(next_state, \"X\") == 1:\n",
    "                return a\n",
    "\n",
    "        # Then, check if a blocking move is necessary\n",
    "        for a in self.actions(state):\n",
    "            next_state = self.game.transition_model(state, a, \"O\")\n",
    "            if self.game.reward(next_state, \"O\") == 1:\n",
    "                return a\n",
    "\n",
    "        # Custom logic to prioritize corners after the center\n",
    "        if state[4] == \"X\":\n",
    "            corner_positions = [0, 2, 6, 8]\n",
    "            unoccupied_corners = [p for p in corner_positions if state[p] == \" \"]\n",
    "            if unoccupied_corners:\n",
    "                return random.choice(unoccupied_corners)\n",
    "\n",
    "        # If neither winning nor blocking is required, return a default action (e.g., the center if available)\n",
    "        valid_actions = self.actions(state)\n",
    "        if valid_actions:\n",
    "            return random.choice(valid_actions)\n",
    "\n",
    "        # If there are no valid actions, return an arbitrary action (0 in this case)\n",
    "        return 0\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for _ in range(num_episodes):\n",
    "            state = (\" \",) * 9\n",
    "            while not self.is_terminal(state):\n",
    "                action = self.choose_action(state)\n",
    "                next_state = self.game.transition_model(state, action, \"X\")\n",
    "                reward = self.game.reward(next_state, \"X\")\n",
    "                next_action = self.choose_action(next_state)\n",
    "\n",
    "                # If next_state leads to a loss, store it in the replay buffer\n",
    "                if reward == -1:\n",
    "                    self.replay_buffer.append(next_state)\n",
    "\n",
    "                # Update Q-value using TD(0) learning\n",
    "                q_state_action = self.q_values.get((state, action), 0)\n",
    "                q_next_state_next_action = self.q_values.get((next_state, next_action), 0)\n",
    "                self.q_values[(state, action)] = q_state_action + self.alpha * (reward + self.gamma * q_next_state_next_action - q_state_action)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "    def play_game(self):\n",
    "        state = (\" \",) * 9  # Initial state\n",
    "        current_player = \"X\"\n",
    "        while not self.is_terminal(state):\n",
    "            self.print_board(state)\n",
    "            if current_player == \"X\":  # \"X\" is the AI player\n",
    "                action = self.choose_action(state)\n",
    "            else:\n",
    "                action = self.human_move(state)\n",
    "\n",
    "            # Update state\n",
    "            state = self.game.transition_model(state, action, current_player)\n",
    "\n",
    "            current_player = \"X\" if current_player == \"O\" else \"O\"\n",
    "\n",
    "        # Final outcome\n",
    "        self.print_board(state)\n",
    "        if self.game.reward(state, \"X\") == 1:\n",
    "            print(\"AI wins!\")\n",
    "        elif self.game.reward(state, \"O\") == 1:\n",
    "            print(\"You win!\")\n",
    "        else:\n",
    "            print(\"It's a draw.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def is_terminal(state):\n",
    "        return TicTacToeEnv.reward(state, \"X\") == 1 or TicTacToeEnv.reward(state, \"O\") == 1 or \" \" not in state\n",
    "\n",
    "    def print_board(self, state):\n",
    "        print(state[0:3])\n",
    "        print(state[3:6])\n",
    "        print(state[6:9])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def human_move(self, state):\n",
    "        while True:\n",
    "            try:\n",
    "                action = int(input(\"Enter your move (0-8): \"))\n",
    "                if action in self.actions(state):\n",
    "                    return action\n",
    "                else:\n",
    "                    print(\"Invalid move. Try again.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number (0-8).\")\n",
    "\n",
    "        return action  # If no winning move, play the selected move\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    game_env = TicTacToeEnv()\n",
    "    game = TicTacToeTD(game_env, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "    game.train(num_episodes=100000)\n",
    "    game.play_game()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
