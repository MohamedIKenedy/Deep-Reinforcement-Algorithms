{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic Tac Toe environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class TicTacToeEnv:\n",
    "    def __init__(self):\n",
    "        # Pre-compute all possible states more efficiently\n",
    "        self.states = list(itertools.product([\"X\", \"O\", \" \"], repeat=9))\n",
    "        self.win_positions = [\n",
    "            (0, 1, 2), (3, 4, 5), (6, 7, 8),  # Rows\n",
    "            (0, 3, 6), (1, 4, 7), (2, 5, 8),  # Columns\n",
    "            (0, 4, 8), (2, 4, 6)  # Diagonals\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def actions(state):\n",
    "        return [i for i, s in enumerate(state) if s == \" \"]\n",
    "\n",
    "    @staticmethod\n",
    "    def transition_model(state, action, player):\n",
    "        return tuple(player if i == action else s for i, s in enumerate(state))\n",
    "\n",
    "    def reward(self, state, player):\n",
    "        # More comprehensive reward function\n",
    "        opponent = \"O\" if player == \"X\" else \"X\"\n",
    "\n",
    "        # Check for win\n",
    "        for pos in self.win_positions:\n",
    "            if state[pos[0]] == state[pos[1]] == state[pos[2]] == player:\n",
    "                return 10  # Higher reward for winning\n",
    "\n",
    "        # Check for draw\n",
    "        if \" \" not in state:\n",
    "            return 0\n",
    "\n",
    "        # Strategic position rewards\n",
    "        reward = 0\n",
    "\n",
    "        # Center control (high strategic value)\n",
    "        if state[4] == player:\n",
    "            reward += 2\n",
    "\n",
    "        # Corners control\n",
    "        corner_positions = [0, 2, 6, 8]\n",
    "        corner_count = sum(1 for i in corner_positions if state[i] == player)\n",
    "        reward += corner_count\n",
    "\n",
    "        # Block opponent's potential win\n",
    "        for pos in self.win_positions:\n",
    "            if sum(1 for i in pos if state[i] == opponent) == 2 and \\\n",
    "               sum(1 for i in pos if state[i] == \" \") == 1:\n",
    "                reward += 3  # High reward for blocking potential win\n",
    "\n",
    "        # Potential win setup\n",
    "        for pos in self.win_positions:\n",
    "            if sum(1 for i in pos if state[i] == player) == 2 and \\\n",
    "               sum(1 for i in pos if state[i] == \" \") == 1:\n",
    "                reward += 4  # Even higher reward for setting up a win\n",
    "\n",
    "        return reward - 5  # Baseline penalty to encourage decisive moves\n",
    "\n",
    "    @staticmethod\n",
    "    def is_terminal(state):\n",
    "        # Simplified terminal state check\n",
    "        win_positions = [(0, 1, 2), (3, 4, 5), (6, 7, 8), (0, 3, 6), (1, 4, 7), (2, 5, 8), (0, 4, 8), (2, 4, 6)]\n",
    "        return any(state[pos[0]] == state[pos[1]] == state[pos[2]] != \" \" for pos in win_positions) or \" \" not in state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' ', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "\n",
      "\n",
      "(' ', ' ', ' ')\n",
      "(' ', 'X', ' ')\n",
      "(' ', ' ', ' ')\n",
      "\n",
      "\n",
      "('O', ' ', ' ')\n",
      "(' ', 'X', ' ')\n",
      "(' ', ' ', ' ')\n",
      "\n",
      "\n",
      "('O', ' ', ' ')\n",
      "(' ', 'X', ' ')\n",
      "(' ', ' ', 'X')\n",
      "\n",
      "\n",
      "('O', ' ', 'O')\n",
      "(' ', 'X', ' ')\n",
      "(' ', ' ', 'X')\n",
      "\n",
      "\n",
      "('O', 'X', 'O')\n",
      "(' ', 'X', ' ')\n",
      "(' ', ' ', 'X')\n",
      "\n",
      "\n",
      "('O', 'X', 'O')\n",
      "(' ', 'X', ' ')\n",
      "('O', ' ', 'X')\n",
      "\n",
      "\n",
      "('O', 'X', 'O')\n",
      "(' ', 'X', 'X')\n",
      "('O', ' ', 'X')\n",
      "\n",
      "\n",
      "('O', 'X', 'O')\n",
      "('O', 'X', 'X')\n",
      "('O', ' ', 'X')\n",
      "\n",
      "\n",
      "Player O wins!\n"
     ]
    }
   ],
   "source": [
    "class ValueIteration:\n",
    "    def __init__(self, theta=0.001, discount_factor=0.9):\n",
    "        self.env = TicTacToeEnv()\n",
    "        self.theta = theta\n",
    "        self.discount_factor = discount_factor\n",
    "        self.states = self.env.states\n",
    "        self.V = {s: 0 for s in self.states}\n",
    "        self.policy = {}\n",
    "\n",
    "    def value_iteration(self):\n",
    "        iterations = 0\n",
    "        max_iterations = 1000  # Prevent infinite loops\n",
    "\n",
    "        while iterations < max_iterations:\n",
    "            delta = 0\n",
    "            for s in self.states:\n",
    "                if self.env.is_terminal(s):\n",
    "                    self.V[s] = 0\n",
    "                    continue\n",
    "\n",
    "                old_value = self.V[s]\n",
    "                \n",
    "                # Compute best action value\n",
    "                action_values = []\n",
    "                for a in self.env.actions(s):\n",
    "                    next_state = self.env.transition_model(s, a, \"X\")\n",
    "                    reward = self.env.reward(next_state, \"X\")\n",
    "                    action_value = reward + self.discount_factor * self.V.get(next_state, 0)\n",
    "                    action_values.append(action_value)\n",
    "\n",
    "                if action_values:\n",
    "                    self.V[s] = max(action_values)\n",
    "                    self.policy[s] = self.env.actions(s)[np.argmax(action_values)]\n",
    "\n",
    "                delta = max(delta, abs(old_value - self.V[s]))\n",
    "\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "            \n",
    "            iterations += 1\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        return self.policy.get(state, random.choice(self.env.actions(state)))\n",
    "\n",
    "    @staticmethod\n",
    "    def print_board(state):\n",
    "        for i in range(0, 9, 3):\n",
    "            print(state[i:i+3])\n",
    "        print(\"\\n\")\n",
    "\n",
    "def play_game():\n",
    "    game = ValueIteration()\n",
    "    game.value_iteration()\n",
    "\n",
    "    state = (\" \",) * 9\n",
    "    current_player = \"X\"\n",
    "\n",
    "    while not TicTacToeEnv.is_terminal(state):\n",
    "        game.print_board(state)\n",
    "\n",
    "        if current_player == \"X\":\n",
    "            while True:\n",
    "                try:\n",
    "                    action = int(input(\"Enter your move (0-8): \"))\n",
    "                    if action in TicTacToeEnv.actions(state):\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Invalid move. Try again.\")\n",
    "                except ValueError:\n",
    "                    print(\"Please enter a number between 0-8.\")\n",
    "\n",
    "        else:\n",
    "            action = game.get_best_action(state)\n",
    "\n",
    "        state = TicTacToeEnv.transition_model(state, action, current_player)\n",
    "        current_player = \"O\" if current_player == \"X\" else \"X\"\n",
    "\n",
    "    game.print_board(state)\n",
    "\n",
    "    # Determine winner\n",
    "    for player in [\"X\", \"O\"]:\n",
    "        if any(state[pos[0]] == state[pos[1]] == state[pos[2]] == player for pos in game.env.win_positions):\n",
    "            print(f\"Player {player} wins!\")\n",
    "            return\n",
    "\n",
    "    print(\"It's a draw!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    play_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' ', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "\n",
      "\n",
      "('X', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "\n",
      "\n",
      "('X', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "(' ', ' ', 'O')\n",
      "\n",
      "\n",
      "('X', ' ', ' ')\n",
      "(' ', 'X', ' ')\n",
      "(' ', ' ', 'O')\n",
      "\n",
      "\n",
      "('X', ' ', 'O')\n",
      "(' ', 'X', ' ')\n",
      "(' ', ' ', 'O')\n",
      "\n",
      "\n",
      "('X', ' ', 'O')\n",
      "(' ', 'X', 'X')\n",
      "(' ', ' ', 'O')\n",
      "\n",
      "\n",
      "('X', ' ', 'O')\n",
      "(' ', 'X', 'X')\n",
      "('O', ' ', 'O')\n",
      "\n",
      "\n",
      "('X', 'X', 'O')\n",
      "(' ', 'X', 'X')\n",
      "('O', ' ', 'O')\n",
      "\n",
      "\n",
      "('X', 'X', 'O')\n",
      "('O', 'X', 'X')\n",
      "('O', ' ', 'O')\n",
      "\n",
      "\n",
      "('X', 'X', 'O')\n",
      "('O', 'X', 'X')\n",
      "('O', 'X', 'O')\n",
      "\n",
      "\n",
      "Player X wins!\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class TicTacToeEnv:\n",
    "    def __init__(self):\n",
    "        self.states = list(itertools.product([\"X\", \"O\", \" \"], repeat=9))\n",
    "        self.win_positions = [\n",
    "            (0, 1, 2), (3, 4, 5), (6, 7, 8),  # Rows\n",
    "            (0, 3, 6), (1, 4, 7), (2, 5, 8),  # Columns\n",
    "            (0, 4, 8), (2, 4, 6)  # Diagonals\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def actions(state):\n",
    "        return [i for i, s in enumerate(state) if s == \" \"]\n",
    "\n",
    "    @staticmethod\n",
    "    def transition_model(state, action, player):\n",
    "        return tuple(player if i == action else s for i, s in enumerate(state))\n",
    "\n",
    "    def evaluate_state(self, state, player):\n",
    "        opponent = \"O\" if player == \"X\" else \"X\"\n",
    "        \n",
    "        # Check for win\n",
    "        for pos in self.win_positions:\n",
    "            if state[pos[0]] == state[pos[1]] == state[pos[2]] == player:\n",
    "                return 100  # High reward for winning\n",
    "        \n",
    "        # Check for opponent win\n",
    "        for pos in self.win_positions:\n",
    "            if state[pos[0]] == state[pos[1]] == state[pos[2]] == opponent:\n",
    "                return -100  # Severe penalty for losing\n",
    "        \n",
    "        # Draw\n",
    "        if \" \" not in state:\n",
    "            return 0\n",
    "        \n",
    "        # Strategic position evaluation\n",
    "        reward = 0\n",
    "        \n",
    "        # Center control\n",
    "        if state[4] == player:\n",
    "            reward += 10\n",
    "        elif state[4] == opponent:\n",
    "            reward -= 10\n",
    "        \n",
    "        # Corner control\n",
    "        corner_positions = [0, 2, 6, 8]\n",
    "        for corner in corner_positions:\n",
    "            if state[corner] == player:\n",
    "                reward += 5\n",
    "            elif state[corner] == opponent:\n",
    "                reward -= 5\n",
    "        \n",
    "        # Potential win and blocking opportunities\n",
    "        for pos in self.win_positions:\n",
    "            player_count = sum(1 for i in pos if state[i] == player)\n",
    "            opponent_count = sum(1 for i in pos if state[i] == opponent)\n",
    "            empty_count = sum(1 for i in pos if state[i] == \" \")\n",
    "            \n",
    "            # Reward for setting up a potential win\n",
    "            if player_count == 2 and empty_count == 1:\n",
    "                reward += 20\n",
    "            \n",
    "            # Penalty for opponent's potential win\n",
    "            if opponent_count == 2 and empty_count == 1:\n",
    "                reward -= 20\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    @staticmethod\n",
    "    def is_terminal(state):\n",
    "        win_positions = [(0, 1, 2), (3, 4, 5), (6, 7, 8), (0, 3, 6), (1, 4, 7), (2, 5, 8), (0, 4, 8), (2, 4, 6)]\n",
    "        return any(state[pos[0]] == state[pos[1]] == state[pos[2]] != \" \" for pos in win_positions) or \" \" not in state\n",
    "\n",
    "class PolicyIteration:\n",
    "    def __init__(self, theta=0.01, discount_factor=0.9, max_iterations=1000):\n",
    "        self.env = TicTacToeEnv()\n",
    "        self.theta = theta\n",
    "        self.discount_factor = discount_factor\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "        # Initialize policy with random actions\n",
    "        self.states = self.env.states\n",
    "        self.V = {s: 0 for s in self.states}\n",
    "        self.pi = {s: random.choice(self.env.actions(s)) if self.env.actions(s) else None for s in self.states}\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        \"\"\"Evaluate the current policy\"\"\"\n",
    "        for _ in range(self.max_iterations):\n",
    "            delta = 0\n",
    "            for s in self.states:\n",
    "                if self.env.is_terminal(s):\n",
    "                    self.V[s] = 0\n",
    "                    continue\n",
    "\n",
    "                if self.pi[s] is None:\n",
    "                    continue\n",
    "\n",
    "                v = self.V[s]\n",
    "                \n",
    "                # Compute value for the current policy\n",
    "                next_state = self.env.transition_model(s, self.pi[s], \"X\")\n",
    "                reward = self.env.evaluate_state(next_state, \"X\")\n",
    "                \n",
    "                self.V[s] = reward + self.discount_factor * self.V.get(next_state, 0)\n",
    "                \n",
    "                delta = max(delta, abs(v - self.V[s]))\n",
    "            \n",
    "            if delta < self.theta:\n",
    "                break\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        \"\"\"Improve the policy based on current value function\"\"\"\n",
    "        policy_stable = True\n",
    "        \n",
    "        for s in self.states:\n",
    "            if self.env.is_terminal(s):\n",
    "                continue\n",
    "\n",
    "            old_action = self.pi[s]\n",
    "            \n",
    "            # Find the best action by looking at all possible actions\n",
    "            action_values = []\n",
    "            for a in self.env.actions(s):\n",
    "                next_state = self.env.transition_model(s, a, \"X\")\n",
    "                reward = self.env.evaluate_state(next_state, \"X\")\n",
    "                action_value = reward + self.discount_factor * self.V.get(next_state, 0)\n",
    "                action_values.append((action_value, a))\n",
    "            \n",
    "            # Select the action with the highest value\n",
    "            if action_values:\n",
    "                self.pi[s] = max(action_values, key=lambda x: x[0])[1]\n",
    "            \n",
    "            if old_action != self.pi[s]:\n",
    "                policy_stable = False\n",
    "        \n",
    "        return policy_stable\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        \"\"\"Main policy iteration algorithm\"\"\"\n",
    "        iterations = 0\n",
    "        while iterations < self.max_iterations:\n",
    "            # Evaluate current policy\n",
    "            self.policy_evaluation()\n",
    "            \n",
    "            # Improve policy\n",
    "            if self.policy_improvement():\n",
    "                break\n",
    "            \n",
    "            iterations += 1\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"Get the best action for a given state\"\"\"\n",
    "        return self.pi.get(state, random.choice(self.env.actions(state)) if self.env.actions(state) else None)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_board(state):\n",
    "        \"\"\"Print the current board state\"\"\"\n",
    "        for i in range(0, 9, 3):\n",
    "            print(state[i:i+3])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def play_game(self):\n",
    "        \"\"\"Play a game of Tic Tac Toe\"\"\"\n",
    "        state = (\" \",) * 9\n",
    "        current_player = \"X\"\n",
    "\n",
    "        while not self.env.is_terminal(state):\n",
    "            self.print_board(state)\n",
    "\n",
    "            if current_player == \"X\":\n",
    "                while True:\n",
    "                    try:\n",
    "                        action = int(input(\"Enter your move (0-8): \"))\n",
    "                        if action in self.env.actions(state):\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"Invalid move. Try again.\")\n",
    "                    except ValueError:\n",
    "                        print(\"Please enter a number between 0-8.\")\n",
    "            else:\n",
    "                action = self.get_best_action(state)\n",
    "\n",
    "            state = self.env.transition_model(state, action, current_player)\n",
    "            current_player = \"O\" if current_player == \"X\" else \"X\"\n",
    "\n",
    "        self.print_board(state)\n",
    "\n",
    "        # Determine winner\n",
    "        for player in [\"X\", \"O\"]:\n",
    "            if any(state[pos[0]] == state[pos[1]] == state[pos[2]] == player for pos in self.env.win_positions):\n",
    "                print(f\"Player {player} wins!\")\n",
    "                return\n",
    "\n",
    "        print(\"It's a draw!\")\n",
    "\n",
    "def main():\n",
    "    game = PolicyIteration()\n",
    "    game.policy_iteration()\n",
    "    game.play_game()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First monte carlo visit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' ', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "\n",
      "\n",
      "(' ', ' ', ' ')\n",
      "(' ', ' ', ' ')\n",
      "(' ', 'X', ' ')\n",
      "\n",
      "\n",
      "(' ', ' ', ' ')\n",
      "(' ', 'O', ' ')\n",
      "(' ', 'X', ' ')\n",
      "\n",
      "\n",
      "('X', ' ', ' ')\n",
      "(' ', 'O', ' ')\n",
      "(' ', 'X', ' ')\n",
      "\n",
      "\n",
      "('X', ' ', ' ')\n",
      "('O', 'O', ' ')\n",
      "(' ', 'X', ' ')\n",
      "\n",
      "\n",
      "('X', ' ', ' ')\n",
      "('O', 'O', ' ')\n",
      "('X', 'X', ' ')\n",
      "\n",
      "\n",
      "('X', 'O', ' ')\n",
      "('O', 'O', ' ')\n",
      "('X', 'X', ' ')\n",
      "\n",
      "\n",
      "('X', 'O', ' ')\n",
      "('O', 'O', ' ')\n",
      "('X', 'X', 'X')\n",
      "\n",
      "\n",
      "AI wins!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "class TicTacToeEnv:\n",
    "    def __init__(self):\n",
    "        self.states = list(itertools.product([\"X\", \"O\", \" \"], repeat=9))\n",
    "        self.win_positions = [\n",
    "            (0, 1, 2), (3, 4, 5), (6, 7, 8),  # Rows\n",
    "            (0, 3, 6), (1, 4, 7), (2, 5, 8),  # Columns\n",
    "            (0, 4, 8), (2, 4, 6)  # Diagonals\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def actions(state):\n",
    "        return [i for i, s in enumerate(state) if s == \" \"]\n",
    "\n",
    "    @staticmethod\n",
    "    def transition_model(state, action, player):\n",
    "        return tuple(player if i == action else s for i, s in enumerate(state))\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return any(\n",
    "            state[pos[0]] == state[pos[1]] == state[pos[2]] != \" \" \n",
    "            for pos in self.win_positions\n",
    "        ) or \" \" not in state\n",
    "\n",
    "    def get_reward(self, state, player):\n",
    "        opponent = \"O\" if player == \"X\" else \"X\"\n",
    "        \n",
    "        # Check for win\n",
    "        for pos in self.win_positions:\n",
    "            if state[pos[0]] == state[pos[1]] == state[pos[2]] == player:\n",
    "                return 1  # Win\n",
    "            if state[pos[0]] == state[pos[1]] == state[pos[2]] == opponent:\n",
    "                return -1  # Loss\n",
    "        \n",
    "        # Draw\n",
    "        if \" \" not in state:\n",
    "            return 0\n",
    "        \n",
    "        return 0  # Ongoing game\n",
    "\n",
    "class MonteCarloTreeSearch:\n",
    "    def __init__(self, num_simulations=10000, exploration_weight=1.4):\n",
    "        self.env = TicTacToeEnv()\n",
    "        self.num_simulations = num_simulations\n",
    "        self.exploration_weight = exploration_weight\n",
    "\n",
    "    def monte_carlo_tree_search(self, initial_state):\n",
    "        \"\"\"Main MCTS algorithm\"\"\"\n",
    "        class Node:\n",
    "            def __init__(self, state, parent=None, action=None):\n",
    "                self.state = state\n",
    "                self.parent = parent\n",
    "                self.action = action\n",
    "                self.children = {}\n",
    "                self.visits = 0\n",
    "                self.value = 0\n",
    "                self.untried_actions = TicTacToeEnv().actions(state)\n",
    "\n",
    "            def fully_expanded(self):\n",
    "                return len(self.untried_actions) == 0\n",
    "\n",
    "            def best_child(self, c_puct):\n",
    "                if not self.children:\n",
    "                    return None\n",
    "                \n",
    "                return max(\n",
    "                    self.children.values(), \n",
    "                    key=lambda child: child.value / (child.visits + 1e-6) + \n",
    "                        c_puct * math.sqrt(math.log(self.visits + 1) / (child.visits + 1e-6))\n",
    "                )\n",
    "\n",
    "        root = Node(initial_state)\n",
    "        current_player = \"X\"\n",
    "\n",
    "        for _ in range(self.num_simulations):\n",
    "            node = root\n",
    "            \n",
    "            # Selection\n",
    "            while not self.env.is_terminal(node.state) and node.fully_expanded():\n",
    "                node = node.best_child(self.exploration_weight)\n",
    "            \n",
    "            # Expansion\n",
    "            if not self.env.is_terminal(node.state):\n",
    "                action = node.untried_actions.pop()\n",
    "                next_state = self.env.transition_model(node.state, action, current_player)\n",
    "                child_node = Node(next_state, parent=node, action=action)\n",
    "                node.children[action] = child_node\n",
    "                node = child_node\n",
    "\n",
    "            # Simulation\n",
    "            simulation_state = node.state\n",
    "            sim_player = current_player\n",
    "            while not self.env.is_terminal(simulation_state):\n",
    "                actions = self.env.actions(simulation_state)\n",
    "                action = random.choice(actions)\n",
    "                simulation_state = self.env.transition_model(simulation_state, action, sim_player)\n",
    "                sim_player = \"O\" if sim_player == \"X\" else \"X\"\n",
    "\n",
    "            # Backpropagation\n",
    "            result = self.env.get_reward(simulation_state, current_player)\n",
    "            while node is not None:\n",
    "                node.visits += 1\n",
    "                node.value += result\n",
    "                node = node.parent\n",
    "\n",
    "        # Return the action with the most visits\n",
    "        return max(root.children, key=lambda a: root.children[a].visits)\n",
    "\n",
    "    def play_game(self):\n",
    "        \"\"\"Interactive game against MCTS AI\"\"\"\n",
    "        state = (\" \",) * 9\n",
    "        current_player = \"X\"\n",
    "\n",
    "        while not self.env.is_terminal(state):\n",
    "            self.print_board(state)\n",
    "\n",
    "            if current_player == \"X\":\n",
    "                # AI's turn\n",
    "                action = self.monte_carlo_tree_search(state)\n",
    "            else:\n",
    "                # Human's turn\n",
    "                action = self.get_human_move(state)\n",
    "\n",
    "            state = self.env.transition_model(state, action, current_player)\n",
    "            current_player = \"O\" if current_player == \"X\" else \"X\"\n",
    "\n",
    "        # Final board and result\n",
    "        self.print_board(state)\n",
    "        \n",
    "        # Determine winner\n",
    "        if any(state[pos[0]] == state[pos[1]] == state[pos[2]] == \"X\" for pos in self.env.win_positions):\n",
    "            print(\"AI wins!\")\n",
    "        elif any(state[pos[0]] == state[pos[1]] == state[pos[2]] == \"O\" for pos in self.env.win_positions):\n",
    "            print(\"You win!\")\n",
    "        else:\n",
    "            print(\"It's a draw!\")\n",
    "\n",
    "    def get_human_move(self, state):\n",
    "        \"\"\"Get valid human move\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                action = int(input(\"Enter your move (0-8): \"))\n",
    "                if action in self.env.actions(state):\n",
    "                    return action\n",
    "                else:\n",
    "                    print(\"Invalid move. Try again.\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a number between 0-8.\")\n",
    "\n",
    "    def print_board(self, state):\n",
    "        \"\"\"Print the current board state\"\"\"\n",
    "        for i in range(0, 9, 3):\n",
    "            print(state[i:i+3])\n",
    "        print(\"\\n\")\n",
    "\n",
    "def main():\n",
    "    mcts = MonteCarloTreeSearch(num_simulations=50000)\n",
    "    mcts.play_game()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
